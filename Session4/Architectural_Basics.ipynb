{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Architectural Basics.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VjTaEZC4Tqy",
        "colab_type": "text"
      },
      "source": [
        "**How many layers**\n",
        "\n",
        "  Ans :- As many as possible to reach the output, as per my understanding it depends on image size where we try to reach the final layes by adding conv layer with appropriate kernal size.\n",
        "  \n",
        "**MaxPooling** \n",
        "\n",
        "   Ans :- while convolving over, max pooling layer extracts the max pixel value as per the size of kernal, we generally use 2x2 size maxpooling. \n",
        "   \n",
        "**1x1 Convolutions**\n",
        "\n",
        "  Ans :- it is like an ant man, we use it when we need to convert layer into 1x1 array, it converts previous layer into 1 dimensional array. \n",
        "  \n",
        "**3x3 Convolutions**\n",
        "\n",
        "  Ans :- it is a 3x3 kernal size convolution layer where receptive field keep on increasing with positive diference of 2 in size.\n",
        " \n",
        "**Receptive Field**\n",
        "\n",
        "  Ans :- receptive field is nothing but view on conv layer and how much part of image the kernal is covering. there are 2 types of receptive field. one local which is dependent on kernal size and same as kernal size on first layer, it keep increasing with size of +2, second is global receptive field which is nothing but size if image.\n",
        "\n",
        "**SoftMax**\n",
        "\n",
        "  Ans :-  it is activation function. also known as Maximum Entropy (MaxEnt) Classifier. The function is usually used to compute losses that can be expected when training a data set.\n",
        "\n",
        "**Learning Rate**\n",
        "\n",
        "  Ans :- its a type of hyperparameter which decides on what manner new changes has to be override.\n",
        "\n",
        "**Kernels and how do we decide the number of kernels?**\n",
        "\n",
        "  Ans :- Kernals are feature extractors, number of kernals decide the paramenters to be trained on.\n",
        "\n",
        "**Batch Normalization**\n",
        "\n",
        " Ans :- BN simply sharpen the kernals so we can reach to right info.\n",
        "\n",
        "**Image Normalization**\n",
        "\n",
        " Ans :- it increases the pixel intensity, can be alternate of BN.\n",
        "\n",
        "**Position of MaxPooling**\n",
        "\n",
        "  Ans :- better to use after 2-3 conv layer, and not to use before last conv layer.\n",
        "\n",
        "**Concept of Transition Layers**\n",
        "\n",
        "  Ans :-  it is cobination of conv layer and pooling layer. a type of block.\n",
        "\n",
        "**Position of Transition Layer**\n",
        "\n",
        "  Ans :- Its a block of conv layers and pooling layer. couple of conv layers with one pooling layer, thats the block we can use.\n",
        "\n",
        "**Number of Epochs and when to increase them**\n",
        "\n",
        "  Ans :- for a certain time no. of epochs matters for accuracy and later on it is keep on decreasing so we have to use the no. in such a way it reaches the max accuracy.\n",
        "  \n",
        "**DropOut**\n",
        "\n",
        "  Ans :- it is regularization technique which help in overfiting.\n",
        "  \n",
        "**When do we introduce DropOut, or when do we know we have some overfitting**\n",
        "\n",
        "  Ans :- we know about overfitting when we are having high accuracy during training but not with validation, we can use dropout after every transition layer.\n",
        "  \n",
        "**The distance of MaxPooling from Prediction**\n",
        "\n",
        "  Ans :- we are not supposed to use max pooling before last conv layer.\n",
        "  \n",
        "**The distance of Batch Normalization from Prediction**\n",
        "\n",
        "  Ans :- BN can be used after every conv layer but not after 1x1 layer.\n",
        "  \n",
        "**When do we stop convolutions and go ahead with a larger kernel or some other alternative (which we have not yet covered)**\n",
        "\n",
        "  Ans :- when image is bigger size and we have super resolution issues.\n",
        "  \n",
        "**How do we know our network is not going well, comparatively, very early**\n",
        "\n",
        "  Ans :- while compile and training, first two val. accuracy can be viewed and can help us to decide it better with the help of parameters as well.\n",
        "  \n",
        "**Batch Size, and effects of batch size**\n",
        "\n",
        "  Ans :- should max batch size when accuracy is stable and better, less batch size is time consuming as well.\n",
        "  \n",
        "**When to add validation checks**\n",
        "\n",
        "  Ans :- it can be added at time of comilation and after every iteration it can be helpfull to see the network performance.\n",
        "  \n",
        "**LR schedule and concept behind it**\n",
        "\n",
        "  Ans :- Learning Rate can be scheduled to controll the speed of learning override, we can use to slow down after every iteration.\n",
        "\n",
        "**Adam vs SGD**\n",
        "\n",
        "  Ans :- both are optimizer function which we use while compiling the NN. SGD refers to Stochastic gradient descent optimizer and it is a variant of gradient descent while Adam is an algorithm for gradient-based optimization of stochastic objective functions. It combines the advantages of two SGD extensions — Root Mean Square Propagation (RMSProp) and Adaptive Gradient Algorithm (AdaGrad) — and computes individual adaptive learning rates for different parameters.\n",
        "  "
      ]
    }
  ]
}